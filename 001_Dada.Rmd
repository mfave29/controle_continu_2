---
title: "000_dada2"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

# Importation des données

```{bash}
wget https://pagesperso.univ-brest.fr/~maignien/teaching/M1-MFA/UE-Ecogenomique2/EcoG2_data_cc2.tar.gz
tar xvzf EcoG2_data_cc2.tar.gz
```

```{r}
path <- "~/controle_continu_2/controle_continu_2/Stratif_CC2"
list.files(path)
```

# Forward and Reverse
```{r}
fnFs <- sort(list.files(path, pattern="_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2.fastq", full.names = TRUE))

sample.names <- sapply(strsplit(basename(fnFs), "_R"), `[`,1)
```



# Profils de score de qualité 
```{r}
plotQualityProfile(fnFs[1:2])
```

Ici sont présentés les profiles des scores de qualité des forwards de deux échantillons différents. Le score de qualité moyen correspond à la ligne verte. La ligne orange correspond aux quartiles de la distribution du score de qualité.  
Les premières lectures sont de bonne qualité. Plus on fait de lectures, moins la qualité est bonne. C'est pour cela qu'on filtrera plus tard. 

Nous faisons la même chose pour les mêmes échantillons, cette fois-ci avec les reverse : 

```{r}
plotQualityProfile(fnRs[1:2])
```

Nous pouvons constater que les read reverse sont de moins bonne qualité, ce qui apparaît souvent avec Illumina. Nous allons également découper. 


#Filtrage et coupures
Tout d'abord, nous attribuons des noms aux fichiers filtrés. Ainsi, les forward filtrés sont sauvegardés sous le nom de filtFs et les reverse filtrés sous le nom de filtRs.

```{r}
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```


```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(250,220), trimLeft=c(21,21), maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE, compress=TRUE, multithread = TRUE)
head(out)
```

Nous avons choisi de couper à 210 pb pour les forward car après, la qualité diminuait. Cela a été fait à 180pb pour les reverse. 
Nous avions vu précédemment, par les graphiques, que la qualité des forward était meilleur que les reverse.
. 

Nous voyons que le filtrage a été effectué, le nombre de reads retenus étant inférieur aux reads avant le filtrage. 

Le paramètre trimLeft a été utilisé afin d'enlever les primers utilisés précédemment. 


# Apprendre le taux d'erreur
Les manipulations suivantes permettront de visualiser l'estimation des taux d'erreur et l'inférence de la composition de l'échantillon, afin de converger pour une meilleure cohérence. Nous faisons cela pour les Forward tout d'abord, puis pour les Reverse.

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```
Il s'agit de nombre de bases qui seront utilisées pour l'apprentisssage du taux d'erreur pour les forward.

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```
Il s'agit du nombre de bases qui seront utilisées pour l'apprentissage du taux d'erreur pour les Reverse.

Nous allons ensuite visualiser le taux d'erreurs, selon des erreurs de bases qui pourraient se produire. Cet exemple concerne les forward.
```{r}
plotErrors(errF, nominalQ=TRUE)
```


Nous voyons que les taux d'erreur ont une tendance à diminuer des lors que l'on obtient une qualité plus forte. Ainsi, nous pouvons considérer d'après ce graphique que tout semble bon. 

 
# Inférence des échantillons
Nous appliquons cela aux forward filtrés et découpés. Cela permet de faire des comparaisons. Nous commençons par les forward puis nous faisons cette manipulation pour les reverse.

```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```


```{r}
dadaRs <- dada(filtRs, err=errR, multithread = TRUE)
```


```{r}
dadaFs[[1]]
```

Ce résultat signifie qu'il y a 937 variants de séquences à partir de 38016 séquences uniques pour le premier échantilon (Forward).



# Fusions des reads
La fusion des forward et des reverse permet d'avoir des séquences débruitées. Cela peut se faire par complémentarité inverse entre les forward et les reverse. Il y a formation de contigs. Il faut absolument qu'il y ait un certain chevauchement. 

La fonction "merge" permet de faire cela, tout en éliminant les paires qui ne se chevauchent pas donc, ou s'il y  trop de divergences dans cette région chevauchante. 
Le fait de mettre "verbose=TRUE" permet d'avoir une sortie sur le document. 

```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
```


# Construction d'une table de séquence
Nous construisons une table ASV, pour Amplicon Sequence Variant table. 
Un variant de séquence d'amplicon est une séquence unique d'ADN. Ces variants sont créés après le filtrage que nous avons effectué auparavant. Cela permet la classification des séquences en groupement d'espèces. Cette méthode est plus précise que les OTU car la première permet de voir ne serait-ce qu'un changement de nucléotide. 

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```

Nous voyons qu'il y a 11099 ASV dans les 11 échantillons. 

## Distribution des longueurs des séquences
```{r}
table(nchar(getSequences(seqtab)))
```
Nous avons les longueurs des séquences. 



# Elimination des chimères
Dada ne permet pas d'éliminer les chimères. Ceci est plus facile à faire à partir d'ASV en comparaison aux OTU. 

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```

```{r}
sum(seqtab.nochim)/sum(seqtab)
```


Nous voyons qu'il y a environ 20% de chimères. 


# Suivi des reads dans le pipeline
Ces tests permettent de voir à quel degré nous avons conservé les reads de départ. Comparer les pertes !!

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```


Cela nous a permis de vérifier la cohérence. Nous voyons donc que le nombre a chuté à l'issue du filtrage. 


# Assignation de la taxonomie
Grâce à la fonction "assignTaxonomy" permet de classer les séquences avec une taxonomie connue. Nous prenons la base de données Silva. 

```{bash}
wget https://zenodo.org/record/1172783/files/silva_nr_v132_train_set.fa.gz
```

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/controle_continu_2/controle_continu_2/silva_nr_v132_train_set.fa.gz", multithread=TRUE)
```




Nous allons regarder les affectations taxonomiques : 
```{r}
taxa.print <- taxa
rownames(taxa.print) <- NULL
head(taxa.print)
```

Nous observons les 6 premiers taxons. 



# Evaluation de la précision
S'il y a une communauté fictive faire

```{r}
unqs.mock <- seqtab.nochim["Mock"]
unqs.mock <- sort(unqs.mock[unqs.mock>0], decreasing=TRUE) # Drop ASVs absent in the Mock
cat("DADA2 inferred", length(unqs.mock), "sample sequences present in the Mock community.\n")
```
```{r}
samples.out <- rownames(seqtab.nochim)
profondeur <- sapply(strsplit(samples.out, "D"), `[`, 1)
date <- substr(profondeur,0,11)
samdf <- data.frame(Profondeur=profondeur, Date=date)
samdf$Profondeur[samdf$Date>11] <- c("Fond","Median","Surface")
samdf$Date[samdf$Profondeur>11] <- c("10sept14","11mars15")
rownames(samdf) <- samples.out
```

```{r}
write.csv(samdf, "samdf.csv")
```

```{r}
samdf <-read.table("~/controle_continu_2/controle_continu_2/samdf.csv", sep=",", header=TRUE, row.names = 1)
```



```{r}
library(phangorn)
library(DECIPHER)
seqs <- getSequences(seqtab.nochim)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA,verbose=FALSE)
phangAlign <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phangAlign)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phangAlign)
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
        rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)
```

```{r}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa),phy_tree(fitGTR$tree))
ps
```
```{r}
plot_richness(ps, x="Date", measures=c("Shannon", "Simpson"), color="Profondeur")
```



```{r}
rank_names(ps)
```

```{r}
table(tax_table(ps)[, "Phylum"], exclude = NULL)
```


```{r}
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
```

```{r}
# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps),
               MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(ps),
                    tax_table(ps))
```

```{r}
plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```

```{r}
# Define phyla to filter
filterPhyla = c("Elusimicrobia", "Epsilonbacteraeota", "Fibrobacteres", "Hydrogenedentes", "Omnitrophicaeota", "PAUC34f")
# Filter entries with unidentified Phylum.
ps1 = subset_taxa(ps, !Phylum %in% filterPhyla)
ps1
```

```{r}
# Subset to the remaining phyla
prevdf1 = subset(prevdf, Phylum %in% get_taxa_unique(ps1, "Phylum"))
ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps),color=Phylum)) +
  # Include a guess for parameter
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) + theme(legend.position="none")
```

```{r}
# Define prevalence threshold as 5% of total samples
prevalenceThreshold = 0.05 * nsamples(ps)
prevalenceThreshold
```

```{r}
# Execute prevalence filter, using `prune_taxa()` function
keepTaxa = rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ps2 = prune_taxa(keepTaxa, ps)
```



```{r}
# How many genera would be present after filtering?
length(get_taxa_unique(ps2, taxonomic.rank = "Genus"))
```

```{r}
ps3 = tax_glom(ps2, "Genus", NArm = TRUE)
```


```{r}
h1 = 0.4
ps4 = tip_glom(ps2, h = h1)
```


```{r}
multiPlotTitleTextSize = 15
p2tree = plot_tree(ps2, method = "treeonly",
                   ladderize = "left",
                   title = "Before Agglomeration") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
p3tree = plot_tree(ps3, method = "treeonly",
                   ladderize = "left", title = "By Genus") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
p4tree = plot_tree(ps4, method = "treeonly",
                   ladderize = "left", title = "By Height") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
```


```{r}
# group plots together
grid.arrange(nrow = 1, p2tree, p3tree, p4tree)
```

```{r}
plot_abundance = function(physeq,title = "",
                          Facet = "Order", Color = "Phylum"){
  # Arbitrary subset, based on Phylum, for plotting
  p1f = subset_taxa(physeq, Phylum %in% c("Actinobacteria"))
  mphyseq = psmelt(p1f)
  mphyseq <- subset(mphyseq, Abundance > 0)
  ggplot(data = mphyseq, mapping = aes_string(x = "Profondeur",y = "Abundance",
                              color = Color, fill = Color)) +
    geom_violin(fill = NA) +
    geom_point(size = 1, alpha = 0.3,
               position = position_jitter(width = 0.3)) +
    facet_wrap(facets = Facet) + scale_y_log10()+
    theme(legend.position="none")
}
```

```{r}
# Transform to relative abundance. Save as new object.
ps3ra = transform_sample_counts(ps3, function(x){x / sum(x)})
```

```{r}
plotBefore = plot_abundance(ps3,"")
plotAfter = plot_abundance(ps3ra,"")
# Combine each plot into one graphic.
grid.arrange(nrow = 2,  plotBefore, plotAfter)
```
```{r}
psOrd = subset_taxa(ps3ra, Order == "Microtrichales")
plot_abundance(psOrd, Facet = "Genus", Color = NULL)
```


```{r}
plot_abundance1 = function(physeq,title = "",
                          Facet = "Order", Color = "Phylum"){
  # Arbitrary subset, based on Phylum, for plotting
  p1f = subset_taxa(physeq, Phylum %in% c("Actinobacteria"))
  mphyseq = psmelt(p1f)
  mphyseq <- subset(mphyseq, Abundance > 0)
  ggplot(data = mphyseq, mapping = aes_string(x = "Date",y = "Abundance",
                              color = Color, fill = Color)) +
    geom_violin(fill = NA) +
    geom_point(size = 1, alpha = 0.3,
               position = position_jitter(width = 0.3)) +
    facet_wrap(facets = Facet) + scale_y_log10()+
    theme(legend.position="none")
}
```

```{r}
# Transform to relative abundance. Save as new object.
ps3ra1 = transform_sample_counts(ps3, function(x){x / sum(x)})
```

```{r}
plotBefore1 = plot_abundance1(ps3,"")
plotAfter1 = plot_abundance1(ps3ra1,"")
# Combine each plot into one graphic.
grid.arrange(nrow = 2,  plotBefore1, plotAfter1)
```

```{r}
psOrd1 = subset_taxa(ps3ra1, Order == "Microtrichales")
plot_abundance1(psOrd1, Facet = "Genus", Color = NULL)
```































